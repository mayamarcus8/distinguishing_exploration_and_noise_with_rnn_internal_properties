{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805c8b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing as mp\n",
    "import pickle \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from imports import*\n",
    "from utils import *\n",
    "from logistic_regression import *\n",
    "from rnn import *\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd4a4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of agent\n",
    "num_of_agents = 40\n",
    "\n",
    "# num of block\n",
    "num_of_block = 3\n",
    "\n",
    "# num of trials \n",
    "num_of_trials = 200\n",
    "\n",
    "# for cross valdation \n",
    "array = np.arange(num_of_block)\n",
    "cv = [np.roll(array,i) for i in range(num_of_block)]\n",
    "cv = np.array(cv)\n",
    "\n",
    "\n",
    "def bce(y_hat,y_true):\n",
    "    eps = 1e-7\n",
    "    return -np.sum( y_true*np.log(y_hat+eps) + (1-y_true)*np.log(1-y_hat+eps) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af18874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data\n",
    "def upload_data(num_of_block,num_of_agents,model):\n",
    "    all_data = [] \n",
    "    for sim in range(1,num_of_block+1):\n",
    "        data_per_block = []\n",
    "        for agent in range(1,num_of_agents+1):\n",
    "            data_per_block.append((pd.read_csv(f'../data/{model}/{model}_agent_{agent}_sim_{sim}.csv')))\n",
    "        all_data.append(data_per_block)\n",
    "        \n",
    "    block_0 = all_data[0]\n",
    "    block_1 = all_data[1]\n",
    "    block_2 = all_data[2]\n",
    "\n",
    "    all_blocks = [block_0,block_1,block_2]\n",
    "\n",
    "    return all_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight_changes_between_states(network_state_t, network_state_t_plus_1):\n",
    "    \"\"\"\n",
    "    Calculate weight changes between two consecutive network states.\n",
    "    \n",
    "    Args:\n",
    "        network_state_t: Network state at time t\n",
    "        network_state_t_plus_1: Network state at time t+1\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Matrix where each row represents flattened weight changes\n",
    "    \"\"\"\n",
    "    weight_changes_list = []\n",
    "    \n",
    "    # Iterate through corresponding parameters in both network states\n",
    "    params_t = dict(network_state_t.named_parameters())\n",
    "    params_t_plus_1 = dict(network_state_t_plus_1.named_parameters())\n",
    "\n",
    "    for name in params_t.keys():     \n",
    "        # Only consider weight matrices, not biases\n",
    "        if 'weight' in name:\n",
    "            # Calculate weight difference and convert to numpy\n",
    "            weight_difference = params_t_plus_1[name].data - params_t[name].data\n",
    "            # Flatten the weight difference matrix into a vector\n",
    "            weight_changes_list.append(weight_difference.flatten().cpu().numpy())\n",
    "            \n",
    "    # Stack vectors vertically to create weight changes matrix\n",
    "    return np.concatenate(weight_changes_list)  \n",
    "\n",
    "def calculate_weight_changes_rank(net, network_states_per_epoch, test_loss, window_size=5):\n",
    "    \"\"\"\n",
    "    Calculate the rank of weight changes matrix around the optimal epoch.\n",
    "    \n",
    "    Args:\n",
    "        net: The trained neural network\n",
    "        network_states_per_epoch: List of network states from training\n",
    "        val_loss: Validation loss array from training\n",
    "        window_size: Number of epochs to consider before and after optimal epoch\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (weight_changes_rank, optimal_epoch_idx)\n",
    "    \"\"\"\n",
    "    # Find optimal epoch based on validation loss\n",
    "    optimal_epoch_idx = np.argmin(test_loss)\n",
    "    \n",
    "    # Calculate weight changes around optimal epoch\n",
    "    weight_changes_matrix_list = []\n",
    "    \n",
    "    # Consider window_size epochs before and after the optimal epoch\n",
    "    start_epoch = max(0, optimal_epoch_idx - window_size)\n",
    "    end_epoch = min(len(network_states_per_epoch) - 1, optimal_epoch_idx + window_size)\n",
    "    \n",
    "    for t in range(start_epoch, end_epoch):\n",
    "        # Create temporary networks for states at t and t+1\n",
    "        network_at_t = GRU_NN(INPUT_SIZE, net.hidden_size, 1, OUTPUT_SIZE).to(device)\n",
    "        network_at_t_plus_1 = GRU_NN(INPUT_SIZE, net.hidden_size, 1, OUTPUT_SIZE).to(device)\n",
    "        \n",
    "        # Load states\n",
    "        network_at_t.load_state_dict(network_states_per_epoch[t])\n",
    "        network_at_t_plus_1.load_state_dict(network_states_per_epoch[t + 1])\n",
    "        \n",
    "        # Compute weight changes between consecutive states\n",
    "        weight_changes = compute_weight_changes_between_states(\n",
    "            network_at_t, network_at_t_plus_1)\n",
    "        weight_changes_matrix_list.append(weight_changes)\n",
    "    \n",
    "    # Combine all weight changes into single matrix\n",
    "    complete_weight_changes_matrix = np.hstack(weight_changes_matrix_list)\n",
    "    \n",
    "    # Calculate rank of weight changes matrix\n",
    "    weight_changes_rank = np.linalg.matrix_rank(complete_weight_changes_matrix)\n",
    "    \n",
    "    return weight_changes_rank, optimal_epoch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e649eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = num_of_agents\n",
    "\n",
    "INPUT_SIZE = 4 # 3 for the action (one-hot format) and 1 for the reward of the chosen action\n",
    "OUTPUT_SIZE = 3 # probabilities of choosing each action in the next trial\n",
    "LERANING_RATE = 0.001\n",
    "\n",
    "hidden_size = 5\n",
    "num_layers = 1\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611ed754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_for_model(all_blocks, model):\n",
    "    loss_train, loss_val, loss_test  = [], [], []\n",
    "    ll_train, ll_val, ll_test = [], [], []\n",
    "    ranks, optimal_epochs = [], []\n",
    "\n",
    "    for n in tqdm(range(N)):\n",
    "        network_states_per_epoch = []\n",
    "        for train, val, test in cv:\n",
    "\n",
    "            train_data = behavior_dataset(all_blocks[train][n])\n",
    "            val_data = behavior_dataset(all_blocks[val][n])\n",
    "            test_data = behavior_dataset(all_blocks[test][n])\n",
    "\n",
    "            train_loader = DataLoader(train_data,shuffle=False,batch_size=len(train_data))\n",
    "            val_loader = DataLoader(val_data,shuffle=False,batch_size=len(val_data))\n",
    "            test_loader = DataLoader(test_data,shuffle=False,batch_size=len(test_data))\n",
    "            \n",
    "            rnn = GRU_NN(INPUT_SIZE, hidden_size, num_layers, OUTPUT_SIZE)\n",
    "            rnn, train_loss, train_ll, val_loss, val_ll, test_loss, test_ll, network_states_per_epoch = train_model(rnn,\n",
    "                                                                                    train_loader,\n",
    "                                                                                    val_loader,\n",
    "                                                                                    test_loader,\n",
    "                                                                                    epochs=epochs,\n",
    "                                                                                    lr=LERANING_RATE) \n",
    "            \n",
    "            rank, optimal_epoch = calculate_weight_changes_rank(rnn, network_states_per_epoch, test_loss)\n",
    "                                                                                                                                        \n",
    "            loss_train.append(train_loss)\n",
    "            loss_val.append(val_loss)\n",
    "            loss_test.append(test_loss)\n",
    "            \n",
    "            ll_train.append(train_ll)\n",
    "            ll_val.append(val_ll)\n",
    "            ll_test.append(test_ll)\n",
    "\n",
    "            ranks.append(rank)\n",
    "            optimal_epochs.append(optimal_epoch)\n",
    "            \n",
    "        print('Done agent',n)\n",
    "        \n",
    "        \n",
    "    with open(f'../results/{model}_loss_train.pickle', 'wb') as handle:\n",
    "        pickle.dump(loss_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'../results/{model}_loss_val.pickle', 'wb') as handle:\n",
    "        pickle.dump(loss_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'../results/{model}_loss_test.pickle', 'wb') as handle:\n",
    "        pickle.dump(loss_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'../results/{model}_ll_train.pickle', 'wb') as handle:\n",
    "        pickle.dump(ll_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(f'../results/{model}_ll_val.pickle', 'wb') as handle:\n",
    "        pickle.dump(ll_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(f'../results/{model}_ll_test.pickle', 'wb') as handle:\n",
    "        pickle.dump(ll_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(f'../results/{model}_ranks.pickle', 'wb') as handle:\n",
    "        pickle.dump(ranks, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(f'../results/{model}_optimal_epochs.pickle', 'wb') as handle:\n",
    "        pickle.dump(optimal_epochs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return loss_train, loss_val, loss_test, ll_train, ll_val, ll_test, ranks, optimal_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef986c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model):\n",
    "    # Load the results from pickle files\n",
    "    file_paths = {\n",
    "        \"loss_train\": f\"../results/{model}_loss_train.pickle\",\n",
    "        \"loss_val\": f\"../results/{model}_loss_val.pickle\",\n",
    "        \"loss_test\": f\"../results/{model}_loss_test.pickle\",\n",
    "        \"ll_train\": f\"../results/{model}_ll_train.pickle\",\n",
    "        \"ll_val\": f\"../results/{model}_ll_val.pickle\",\n",
    "        \"ll_test\": f\"../results/{model}_ll_test.pickle\",\n",
    "        \"ranks\": f\"../results/{model}_ranks.pickle\",\n",
    "        \"optimal_epochs\": f\"../results/{model}_optimal_epochs.pickle\"\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for key, path in file_paths.items():\n",
    "        with open(path, 'rb') as handle:\n",
    "            results[key] = pickle.load(handle)\n",
    "\n",
    "    # Convert lists of lists to averaged lists per epoch\n",
    "    epochs = len(results[\"loss_train\"][0])  # Assuming all have the same epoch length\n",
    "    avg_results = {key: [sum(epoch) / len(epoch) for epoch in zip(*values)] for key, values in results.items()}\n",
    "\n",
    "    # Plotting loss and log-likelihood\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    axes[0].plot(range(epochs), avg_results[\"loss_train\"], label=\"Train Loss\", marker=\"o\")\n",
    "    axes[0].plot(range(epochs), avg_results[\"loss_val\"], label=\"Validation Loss\", marker=\"s\")\n",
    "    axes[0].plot(range(epochs), avg_results[\"loss_test\"], label=\"Test Loss\", marker=\"^\")\n",
    "    axes[0].set_title(\"Loss Over Epochs\")\n",
    "    axes[0].set_xlabel(\"Epochs\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Log-likelihood plot\n",
    "    axes[1].plot(range(epochs), avg_results[\"ll_train\"], label=\"Train Log-Likelihood\", marker=\"o\")\n",
    "    axes[1].plot(range(epochs), avg_results[\"ll_val\"], label=\"Validation Log-Likelihood\", marker=\"s\")\n",
    "    axes[1].plot(range(epochs), avg_results[\"ll_test\"], label=\"Test Log-Likelihood\", marker=\"^\")\n",
    "    axes[1].set_title(\"Log-Likelihood Over Epochs\")\n",
    "    axes[1].set_xlabel(\"Epochs\")\n",
    "    axes[1].set_ylabel(\"Log-Likelihood\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb03d0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [01:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 60 and the array at index 1 has size 75",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m      7\u001b[0m     all_blocks \u001b[38;5;241m=\u001b[39m upload_data(num_of_block,num_of_agents,model)\n\u001b[1;32m----> 8\u001b[0m     loss_train, loss_val, loss_test, ll_train, ll_val, ll_test, ranks, optimal_epochs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rnn_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     plot_results(model)\n\u001b[0;32m     10\u001b[0m     optimal_epochs_per_model[model] \u001b[38;5;241m=\u001b[39m optimal_epochs\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mtrain_rnn_for_model\u001b[1;34m(all_blocks, model)\u001b[0m\n\u001b[0;32m     18\u001b[0m rnn \u001b[38;5;241m=\u001b[39m GRU_NN(INPUT_SIZE, hidden_size, num_layers, OUTPUT_SIZE)\n\u001b[0;32m     19\u001b[0m rnn, train_loss, train_ll, val_loss, val_ll, test_loss, test_ll, network_states_per_epoch \u001b[38;5;241m=\u001b[39m train_model(rnn,\n\u001b[0;32m     20\u001b[0m                                                                         train_loader,\n\u001b[0;32m     21\u001b[0m                                                                         val_loader,\n\u001b[0;32m     22\u001b[0m                                                                         test_loader,\n\u001b[0;32m     23\u001b[0m                                                                         epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m     24\u001b[0m                                                                         lr\u001b[38;5;241m=\u001b[39mLERANING_RATE) \n\u001b[1;32m---> 26\u001b[0m rank, optimal_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_weight_changes_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_states_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m loss_train\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     29\u001b[0m loss_val\u001b[38;5;241m.\u001b[39mappend(val_loss)\n",
      "Cell \u001b[1;32mIn[4], line 62\u001b[0m, in \u001b[0;36mcalculate_weight_changes_rank\u001b[1;34m(net, network_states_per_epoch, test_loss, window_size)\u001b[0m\n\u001b[0;32m     59\u001b[0m     network_at_t_plus_1\u001b[38;5;241m.\u001b[39mload_state_dict(network_states_per_epoch[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Compute weight changes between consecutive states\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     weight_changes \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_weight_changes_between_states\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnetwork_at_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_at_t_plus_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     weight_changes_matrix_list\u001b[38;5;241m.\u001b[39mappend(weight_changes)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Combine all weight changes into single matrix\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mcompute_weight_changes_between_states\u001b[1;34m(network_state_t, network_state_t_plus_1)\u001b[0m\n\u001b[0;32m     24\u001b[0m         weight_changes_list\u001b[38;5;241m.\u001b[39mappend(weight_difference\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Stack vectors vertically to create weight changes matrix\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_changes_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32md:\\17.06.2024\\Projects\\distinguishing_exploration_and_noise_with_rnn_internal_properties\\.venv\\lib\\site-packages\\numpy\\core\\shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    295\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 60 and the array at index 1 has size 75"
     ]
    }
   ],
   "source": [
    "models = [\"none\", \"exploration_only\", \"noise_only\", \"both\"]\n",
    "optimal_epochs_per_model = {}\n",
    "ranks_per_model = {}\n",
    "test_losses_per_model = {}\n",
    "\n",
    "for model in models:\n",
    "    all_blocks = upload_data(num_of_block,num_of_agents,model)\n",
    "    loss_train, loss_val, loss_test, ll_train, ll_val, ll_test, ranks, optimal_epochs = train_rnn_for_model(all_blocks, model)\n",
    "    plot_results(model)\n",
    "    optimal_epochs_per_model[model] = optimal_epochs\n",
    "    ranks_per_model[model] = ranks\n",
    "    test_losses_per_model[model] = loss_test\n",
    "\n",
    "#plot optimal epochs by model\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for model, optimal_epochs in optimal_epochs_per_model.items():\n",
    "    sns.histplot(optimal_epochs, kde=True, label=model, ax=ax)\n",
    "ax.set_title(\"Optimal Epochs\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plot ranks by model\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for model, ranks in ranks_per_model.items():\n",
    "    sns.histplot(ranks, kde=True, label=model, ax=ax)\n",
    "ax.set_title(\"Rank of Weight Changes Matrix Around Optimal Epoch\")\n",
    "ax.set_xlabel(\"Rank\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#plot test losses by epoch, grouped by model\n",
    "# fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# for model, test_losses in test_losses_per_model.items():\n",
    "#     for test_loss in test_losses:\n",
    "#         ax.plot(range(len(test_loss)), test_loss, label=model)\n",
    "# ax.set_title(\"Test Loss Over Epochs\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
