{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805c8b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing as mp\n",
    "import pickle \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from imports import*\n",
    "from utils import *\n",
    "from logistic_regression import *\n",
    "from rnn import *\n",
    "\n",
    "from hybrid_sim import *\n",
    "from hybrid_fit import *\n",
    "from hybrid_predict import *\n",
    "\n",
    "from habit_sim import *\n",
    "from habit_fit import *\n",
    "from habit_predict import *\n",
    "\n",
    "from kdh_sim import *\n",
    "from kdh_fit import *\n",
    "from kdh_predict import *\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd4a4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of agent\n",
    "num_of_agents = 100\n",
    "\n",
    "# num of block\n",
    "num_of_block = 3\n",
    "\n",
    "# num of trials \n",
    "num_of_trials = 200\n",
    "\n",
    "# for cross valdation \n",
    "array = np.arange(num_of_block)\n",
    "cv = [np.roll(array,i) for i in range(num_of_block)]\n",
    "cv = np.array(cv)\n",
    "\n",
    "models = {\n",
    "    \n",
    "    'habit':[  configuration_parameters_habit,\n",
    "               habit_sim,\n",
    "               habit_fit,\n",
    "               habit_predict]\n",
    "}\n",
    "\n",
    "\n",
    "def bce(y_hat,y_true):\n",
    "    eps = 1e-7\n",
    "    return -np.sum( y_true*np.log(y_hat+eps) + (1-y_true)*np.log(1-y_hat+eps) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f3d57fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model habit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 14.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# sim\n",
    "for m in models:\n",
    "    print(f'Model {m}')\n",
    "    \n",
    "    data_per_agent = []\n",
    "    parameters = []\n",
    "\n",
    "    for agent in tqdm(range(num_of_agents)):\n",
    "        param = models[m][0]()\n",
    "        parameters.append(param)\n",
    "        \n",
    "        data = []\n",
    "        for i in range(num_of_block):\n",
    "            # create rewards probs \n",
    "            reward_probs = create_rndwlk(4,num_of_trials)\n",
    "            df = models[m][1](\n",
    "                            param,\n",
    "                            num_of_trials,\n",
    "                            reward_probs\n",
    "            ) \n",
    "            data.append(df)\n",
    "        data_per_agent.append(data)\n",
    "\n",
    "    df = pd.DataFrame(parameters)\n",
    "    df.to_csv(f'../results/{m}/{m}_parameters.csv')\n",
    "    \n",
    "    for agent in range(num_of_agents):\n",
    "        for block in range(num_of_block):\n",
    "            data_per_agent[agent][block].to_csv(f'../data/{m}/{m}_agent_{agent}_sim_{block}.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0af18874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# upload data\n",
    "all_data = [] \n",
    "for sim in range(num_of_block):\n",
    "    data_per_block = []\n",
    "    for m in tqdm(models): \n",
    "        for agent in range(num_of_agents):\n",
    "            data_per_block.append((pd.read_csv(f'../data/{m}/{m}_agent_{agent}_sim_{sim}.csv')))\n",
    "    all_data.append(data_per_block)\n",
    "    \n",
    "block_0 = all_data[0]\n",
    "block_1 = all_data[1]\n",
    "block_2 = all_data[2]\n",
    "\n",
    "all_blocks = [block_0,block_1,block_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1e32ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Fit with habit ***\n",
      "*** train 0 | val 1 | test 2 ***\n",
      "*** train 2 | val 0 | test 1 ***\n",
      "*** train 1 | val 2 | test 0 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:44<00:00, 104.90s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 78\u001b[0m\n\u001b[0;32m     74\u001b[0m         data_results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_nll_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(best_test)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m data_results:\n\u001b[1;32m---> 78\u001b[0m     data_results[k] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_results\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m df_the \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data_results)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "data_results = {\n",
    "    \n",
    "   'agent': [],\n",
    "   'model': [],\n",
    "   'train_block': [],\n",
    "    \n",
    "   'train_nll_hybrid' : [],\n",
    "   'val_nll_hybrid' : [],\n",
    "   'test_nll_hybrid': [], \n",
    "\n",
    "   'train_nll_habit' : [], \n",
    "   'val_nll_habit' : [],\n",
    "   'test_nll_habit': [], \n",
    "\n",
    "   'train_nll_kdh' : [], \n",
    "   'val_nll_kdh' : [],\n",
    "   'test_nll_kdh': [], \n",
    "    \n",
    "}\n",
    "\n",
    "all_best_param = []\n",
    "\n",
    "\n",
    "K = 5\n",
    "N = num_of_agents*1 # 3 models\n",
    "data_results['agent'].append(np.tile(np.arange(0,N),len(cv)))\n",
    "data_results['train_block'].append(np.repeat(cv[:,0],N))\n",
    "data_results['model'].append(np.tile(np.repeat(['hybrid','habit','kdh'],num_of_agents),num_of_block))\n",
    "\n",
    "\n",
    "for m in tqdm(models): \n",
    "\n",
    "    print(f'*** Fit with {m} ***')\n",
    "    \n",
    "    for train, val, test in cv:\n",
    "        print(f'*** train {train} | val {val} | test {test} ***')\n",
    "        \n",
    "        # fit k times \n",
    "        fit_res = []\n",
    "        for _ in range(K):\n",
    "            pool = mp.Pool(processes=mp.cpu_count())\n",
    "            fit = pool.map(models[m][2], all_blocks[train])\n",
    "            pool.close()\n",
    "            fit_res.append(fit)\n",
    "            \n",
    "        # best train/validation nll    \n",
    "        all_nll_train = np.zeros(shape=(K,N))\n",
    "        all_nll_val = np.zeros(shape=(K,N))\n",
    "        all_nll_test = np.zeros(shape=(K,N))\n",
    "        best_parameters = [] \n",
    "        for k in range(K):\n",
    "            for n in range(N):\n",
    "                _ , y_hat, _ = models[m][3](all_blocks[train][n], fit_res[k][n].x)\n",
    "                nLL = bce(1-y_hat, all_blocks[train][n]['action_stage_1'].values)\n",
    "                all_nll_train[k,n] =  nLL\n",
    "                \n",
    "                _ , y_hat, _ = models[m][3](all_blocks[val][n], fit_res[k][n].x)\n",
    "                nLL = bce(1-y_hat, all_blocks[val][n]['action_stage_1'].values)\n",
    "                all_nll_val[k,n] = nLL\n",
    "                \n",
    "                _ , y_hat, _ = models[m][3](all_blocks[test][n], fit_res[k][n].x)\n",
    "                nLL = bce(1-y_hat, all_blocks[test][n]['action_stage_1'].values)\n",
    "                all_nll_test[k,n] = nLL\n",
    "                \n",
    "        best_train = all_nll_train.min(axis=0)\n",
    "        best_val = all_nll_val.min(axis=0)\n",
    "        indx = np.argmin(all_nll_val,axis=0)\n",
    "        best_test = np.array([all_nll_test[indx[n],n] for n in range(N)])\n",
    "        best_param = np.array([fit_res[indx[n]][n].x for n in range(N)])\n",
    "\n",
    "        all_best_param.append(best_param)\n",
    "        data_results[f'train_nll_{m}'].append(best_train)\n",
    "        data_results[f'val_nll_{m}'].append(best_val)\n",
    "        data_results[f'test_nll_{m}'].append(best_test)\n",
    "        \n",
    "\n",
    "for k in data_results:\n",
    "    data_results[k] = np.concatenate(data_results[k])\n",
    "df_the = pd.DataFrame(data_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "N = num_of_agents*1 # 3 models\n",
    "\n",
    "data_results_lr = {\n",
    "        \n",
    "   'train_nll_lr' : [],\n",
    "   'val_nll_lr' : [],\n",
    "   'test_nll_lr': [], \n",
    "    \n",
    "}\n",
    "\n",
    "for train, val, test in cv:\n",
    "    print(f'*** train {train} | val {val} | test {test} ***')\n",
    "    \n",
    "    all_nll_train = np.zeros(shape=(K,N))\n",
    "    all_nll_val = np.zeros(shape=(K,N))\n",
    "    all_nll_test = np.zeros(shape=(K,N))\n",
    "    \n",
    "    fit_res = []    \n",
    "    for k in range(K):\n",
    "        cur_res = []\n",
    "        for n in range(N):\n",
    "            X, y = preprocess_logistic_regression(all_blocks[train][n],lag=k+1)\n",
    "            clf, inter, coef = fit_logistic_regression(X,y)\n",
    "            cur_res.append(clf)\n",
    "        fit_res.append(cur_res)\n",
    "            \n",
    "    # best train/validation nll    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            clf = fit_res[k][n]\n",
    "            \n",
    "            # train\n",
    "            X, y = preprocess_logistic_regression(all_blocks[train][n],lag=k+1)\n",
    "            if clf == None:\n",
    "                nLL = -np.log(.5)*num_of_trials\n",
    "            else:\n",
    "                y_hat = clf.predict_proba(X)[:,0]\n",
    "                nLL = bce(1-y_hat, all_blocks[train][n]['action_stage_1'].values)\n",
    "            \n",
    "            all_nll_train[k,n] = nLL\n",
    "            \n",
    "            # validation\n",
    "            X, y = preprocess_logistic_regression(all_blocks[val][n],lag=k+1)\n",
    "            if clf == None:\n",
    "                nLL = -np.log(.5)*num_of_trials\n",
    "            else:\n",
    "                y_hat = clf.predict_proba(X)[:,0]\n",
    "                nLL = bce(1-y_hat, all_blocks[val][n]['action_stage_1'].values)\n",
    "            \n",
    "            all_nll_val[k,n] = nLL\n",
    "\n",
    "            X, y = preprocess_logistic_regression(all_blocks[test][n],lag=k+1)            \n",
    "            if clf == None:\n",
    "                nLL = -np.log(.5)*num_of_trials\n",
    "            else:\n",
    "                y_hat = clf.predict_proba(X)[:,0]\n",
    "                nLL = bce(1-y_hat, all_blocks[test][n]['action_stage_1'].values)\n",
    "                \n",
    "            all_nll_test[k,n] = nLL\n",
    "            \n",
    "    best_train = all_nll_train.min(axis=0)\n",
    "    best_val = all_nll_val.min(axis=0)\n",
    "    indx = np.argmin(all_nll_val,axis=0)\n",
    "    best_test = np.array([all_nll_test[indx[n],n] for n in range(N)])\n",
    "    \n",
    "    data_results_lr[f'train_nll_lr'].append(best_train)\n",
    "    data_results_lr[f'val_nll_lr'].append(best_val)\n",
    "    data_results_lr[f'test_nll_lr'].append(best_test)\n",
    "    \n",
    "for k in data_results_lr:\n",
    "    data_results_lr[k] = np.concatenate(data_results_lr[k])\n",
    "df_lr = pd.DataFrame(data_results_lr)\n",
    "\n",
    "\n",
    "df = pd.concat([df_the,df_lr],axis=1)\n",
    "df.to_csv('../results/theory_lr.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e649eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = num_of_agents*1 # 3 models\n",
    "\n",
    "INPUT_SIZE = 5\n",
    "OUTPUT_SIZE = 2\n",
    "LERANING_RATE = 0.001\n",
    "\n",
    "hidden_size = 5\n",
    "num_layers = 1\n",
    "epochs = 1000\n",
    "\n",
    "loss_train, loss_val, loss_test  = [], [], []\n",
    "ll_train, ll_val, ll_test = [], [], []\n",
    "\n",
    "for n in tqdm(range(N)):\n",
    "    for train, val, test in cv:\n",
    "\n",
    "        train_data = behavior_dataset(all_blocks[train][n])\n",
    "        val_data = behavior_dataset(all_blocks[val][n])\n",
    "        test_data = behavior_dataset(all_blocks[test][n])\n",
    "\n",
    "        train_loader = DataLoader(train_data,shuffle=False,batch_size=len(train_data))\n",
    "        val_loader = DataLoader(val_data,shuffle=False,batch_size=len(val_data))\n",
    "        test_loader = DataLoader(test_data,shuffle=False,batch_size=len(test_data))\n",
    "        \n",
    "        rnn = GRU_NN(INPUT_SIZE, hidden_size, num_layers, OUTPUT_SIZE)\n",
    "        rnn, train_loss, train_ll, val_loss, val_ll, test_loss, test_ll = train_model(rnn,\n",
    "                                                                                train_loader,\n",
    "                                                                                val_loader,\n",
    "                                                                                test_loader,\n",
    "                                                                                epochs=epochs,\n",
    "                                                                                lr=LERANING_RATE) \n",
    "                                                                                                                                       \n",
    "        loss_train.append(train_loss)\n",
    "        loss_val.append(val_loss)\n",
    "        loss_test.append(test_loss)\n",
    "        \n",
    "        ll_train.append(train_ll)\n",
    "        ll_val.append(val_ll)\n",
    "        ll_test.append(test_ll)\n",
    "        \n",
    "    print('Done agent',n)\n",
    "    \n",
    "    \n",
    "with open('../results/loss_train.pickle', 'wb') as handle:\n",
    "    pickle.dump(loss_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../results/loss_val.pickle', 'wb') as handle:\n",
    "    pickle.dump(loss_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../results/loss_test.pickle', 'wb') as handle:\n",
    "    pickle.dump(loss_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../results/ll_train.pickle', 'wb') as handle:\n",
    "    pickle.dump(ll_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../results/ll_val.pickle', 'wb') as handle:\n",
    "    pickle.dump(ll_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../results/ll_test.pickle', 'wb') as handle:\n",
    "    pickle.dump(ll_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
